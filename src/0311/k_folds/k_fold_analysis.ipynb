{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zBt0n0EZLVO",
    "outputId": "6e9e44a1-d4ec-4ca4-f089-7102126a2f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2563044\n",
      "Academic license 2563044 - for non-commercial use only - registered to 11___@g.nccu.edu.tw\n"
     ]
    }
   ],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import gurobipy_pandas as gppd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gurobipy import min_, max_\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"WLSACCESSID\": \"73a6e3bf-2a9d-41e8-85eb-dd9b9eda802b\",\n",
    "    \"WLSSECRET\": \"c394298a-96ea-4c8c-9d5e-ef2bd5032427\",\n",
    "    \"LICENSEID\": 2563044,\n",
    "}\n",
    "\n",
    "env = gp.Env(params=params)\n",
    "model = gp.Model(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTASer1nQ6iQ"
   },
   "source": [
    "# Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2lZY1EXmRAie"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "\n",
    "salvage_value = 0\n",
    "cost = 300\n",
    "price = 1000\n",
    "holding_cost = 0\n",
    "\n",
    "model_prefix = f\"med_with_holding_cost_{holding_cost}\"\n",
    "\n",
    "#####################\n",
    "\n",
    "CHUNK_SIZE = 100\n",
    "data_size = CHUNK_SIZE * 3\n",
    "train_size = 0.5\n",
    "testing_size = 0.5\n",
    "\n",
    "T = 10\n",
    "service_level = 0.95  # æœå‹™æ°´æº–\n",
    "M = 5000000\n",
    "LASSO_BETA = 100\n",
    "\n",
    "ASSIGNED_FS = np.arange(0.1, 1.0, 0.1)\n",
    "ASSIGNED_TS = list(range(2, T))  # 2 åˆ° T-1\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Gurobi Model Constants\n",
    "THREADS = 12\n",
    "TIME_LIMIT = 20000\n",
    "MIPGAP = 0.01\n",
    "CURRENT_TIMESTAMP = int(datetime.now().strftime(\"%Y%m%d%H%M\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRmALsClGzQB"
   },
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models' Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_parameters(\n",
    "    name: str,\n",
    "    alpha_values=None,\n",
    "    beta_values=None,\n",
    "    f_values=None,\n",
    "    tau_values=None,\n",
    "    data_size=data_size,\n",
    "    current_timestamp=CURRENT_TIMESTAMP,\n",
    "):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    params = {}\n",
    "    if alpha_values is not None:\n",
    "        params[\"alpha\"] = alpha_values\n",
    "    if beta_values is not None:\n",
    "        params[\"beta\"] = beta_values\n",
    "    if f_values is not None:\n",
    "        params[\"f_values\"] = f_values\n",
    "    if tau_values is not None:\n",
    "        params[\"tau_values\"] = tau_values\n",
    "\n",
    "    # å¦‚æœæœ‰åƒæ•¸æ‰é€²è¡Œä¿å­˜\n",
    "    if params:\n",
    "        with open(f\"models/{name}_{data_size}_{current_timestamp}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(params, f)\n",
    "        print(\n",
    "            f\"Model parameters saved as models/{name}_{data_size}_{current_timestamp}.pkl\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No parameters provided to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model_parameters(name: str, data_size: int):\n",
    "    # æ§‹å»ºæª”æ¡ˆçš„è·¯å¾‘\n",
    "    file_path = f\"models/{name}_{data_size}_{CURRENT_TIMESTAMP}.pkl\"\n",
    "\n",
    "    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Model parameters file '{file_path}' has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_models(model_prefix):\n",
    "    file_paths = sorted(glob.glob(f\"models/{model_prefix}_*.pkl\"))\n",
    "\n",
    "    # é€ä¸€è®€å–ä¸¦æ‰“å°æ¯å€‹æª”æ¡ˆçš„å…§å®¹\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            print(f\"Contents of {file_path}:\")\n",
    "            print(params)\n",
    "            print()  # ç©ºè¡Œåˆ†éš”æ¯å€‹æª”æ¡ˆçš„å…§å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_strategies_profits_scatter(save_type, dfs: dict):\n",
    "    names = list(dfs.keys())\n",
    "    df_list = [dfs[name] for name in names]\n",
    "\n",
    "    if len(df_list) <= 1:\n",
    "        print(\"No dataframes to plot.\")\n",
    "        return\n",
    "\n",
    "    pairs = list(itertools.combinations(range(len(df_list)), 2))\n",
    "    num_pairs = len(pairs)\n",
    "    grid_size = math.ceil(math.sqrt(num_pairs))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    fig.suptitle(\"Scatter Plots of Profits (Matrix View)\")\n",
    "\n",
    "    for idx, (i, j) in enumerate(pairs):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        df_i, df_j = df_list[i], df_list[j]\n",
    "\n",
    "        if df_i is None or df_j is None or df_i.empty or df_j.empty:\n",
    "            continue\n",
    "        if len(df_i) != len(df_j):\n",
    "            continue\n",
    "\n",
    "        ax = axes[row, col]\n",
    "        ax.scatter(df_i[\"profits\"], df_j[\"profits\"], alpha=0.6)\n",
    "        ax.plot(\n",
    "            [\n",
    "                min(df_i[\"profits\"].min(), df_j[\"profits\"].min()),\n",
    "                max(df_i[\"profits\"].max(), df_j[\"profits\"].max()),\n",
    "            ],\n",
    "            [\n",
    "                min(df_i[\"profits\"].min(), df_j[\"profits\"].min()),\n",
    "                max(df_i[\"profits\"].max(), df_j[\"profits\"].max()),\n",
    "            ],\n",
    "            \"k--\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "        ax.set_xlabel(names[i])\n",
    "        ax.set_ylabel(names[j])\n",
    "        ax.set_title(f\"{names[i]} vs {names[j]}\")\n",
    "\n",
    "    # Remove empty subplots\n",
    "    for idx in range(num_pairs, grid_size * grid_size):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    save_path = f\"plots/plot_strategies_profits_scatter_{save_type}.png\"\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    print(f\"Plot saved as {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_profit_deviation(save_type, baseline_profit, max_profits):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½å¤šå€‹ç­–ç•¥ç›¸å°æ–¼åŸºæº–çš„å¹³å‡åˆ©æ½¤åå·®ã€‚\n",
    "\n",
    "    :param baseline_profit: åŸºæº–åˆ©æ½¤å€¼\n",
    "    :param max_profits: å„ç­–ç•¥çš„æœ€å¤§åˆ©æ½¤åˆ—è¡¨ï¼ŒåŒ…å« None å€¼æˆ– -1 è¡¨ç¤ºç„¡æ•ˆæ•¸æ“š\n",
    "    \"\"\"\n",
    "    print(f\"Baseline is: {baseline_profit}\")\n",
    "    for i, profit in enumerate(max_profits):\n",
    "        print(f\"S{i+1}'s profit: {profit}\")\n",
    "\n",
    "    # è¨ˆç®—ç›¸å°å€¼\n",
    "    ratios = {}\n",
    "    for idx, max_profit in enumerate(max_profits, start=1):\n",
    "        if max_profit is not None and max_profit != -1:\n",
    "            if baseline_profit != 0:\n",
    "                ratio = (max_profit - baseline_profit) / abs(baseline_profit)\n",
    "                ratios[f\"S{idx}\"] = ratio\n",
    "            else:\n",
    "                # åŸºæº–åˆ©æ½¤ç‚ºé›¶æ™‚ï¼Œç›´æ¥è¨˜éŒ„å¢é‡\n",
    "                ratio = max_profit\n",
    "                ratios[f\"S{idx}\"] = ratio\n",
    "\n",
    "    # è¨­ç½® y è»¸ç¯„åœ\n",
    "    if ratios:\n",
    "        y_min = min(ratios.values()) - 0.1\n",
    "        y_max = max(ratios.values()) + 0.1\n",
    "    else:\n",
    "        y_min, y_max = -0.1, 0.1\n",
    "\n",
    "    # å‰µå»ºåœ–è¡¨é¡¯ç¤ºçµæœ\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    if ratios:\n",
    "        bars = plt.bar(\n",
    "            ratios.keys(), ratios.values(), color=plt.cm.tab10(range(len(ratios)))\n",
    "        )\n",
    "\n",
    "        # åœ¨æ¯å€‹æŸ±ç‹€åœ–ä¸Šæ¨™å‡ºæ•¸å€¼\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                yval,\n",
    "                f\"{yval:.4f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    # æ·»åŠ åŸºæº–ç·šï¼Œè¡¨ç¤ºåŸºæº–å€¼ï¼ˆNo Optï¼‰\n",
    "    plt.axhline(y=0, color=\"gray\", linestyle=\"--\", label=\"Baseline (No Opt)\")\n",
    "\n",
    "    # è¨­ç½®åœ–è¡¨æ¨™é¡Œå’Œè»¸æ¨™ç±¤\n",
    "    plt.title(\"Relative Avg Profit Deviation from Baseline (1)\")\n",
    "    plt.xlabel(\"Strategies\")\n",
    "    plt.ylabel(\"Deviation from Baseline (1)\")\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.legend()\n",
    "\n",
    "    name = \"plot_relative_profit_deviation\"\n",
    "\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    save_path = f\"plots/{name}_{save_type}_{data_size}_{CURRENT_TIMESTAMP}.png\"\n",
    "\n",
    "    plt.savefig(save_path, format=\"png\", bbox_inches=\"tight\")\n",
    "    print(f\"Plot saved as {save_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_profit_comparison(\n",
    "    save_type,\n",
    "    train_baseline_profit,\n",
    "    test_baseline_profit,\n",
    "    test_max_profits,\n",
    "    train_max_profits,\n",
    "):\n",
    "\n",
    "    # Calculate relative deviations from baseline for test and train data\n",
    "    test_ratios, train_ratios = {}, {}\n",
    "    for idx, (test_profit, train_profit) in enumerate(\n",
    "        zip(test_max_profits, train_max_profits), start=1\n",
    "    ):\n",
    "        if test_profit is not None and test_profit != -1:\n",
    "            if test_baseline_profit != 0:\n",
    "                test_ratio = (test_profit - test_baseline_profit) / abs(\n",
    "                    test_baseline_profit\n",
    "                )  # Relative deviation\n",
    "            else:\n",
    "                test_ratio = test_profit  # Use profit directly if baseline is zero\n",
    "            test_ratios[f\"S{idx}\"] = test_ratio\n",
    "\n",
    "        if train_profit is not None and train_profit != -1:\n",
    "            if train_baseline_profit != 0:\n",
    "                train_ratio = (train_profit - train_baseline_profit) / abs(\n",
    "                    train_baseline_profit\n",
    "                )  # Relative deviation\n",
    "            else:\n",
    "                train_ratio = train_profit  # Use profit directly if baseline is zero\n",
    "            train_ratios[f\"S{idx}\"] = train_ratio\n",
    "\n",
    "    # Define the fixed range of the y-axis\n",
    "    max_value = max(\n",
    "        max(test_ratios.values(), default=0), max(train_ratios.values(), default=0)\n",
    "    )\n",
    "    y_max = min(max_value + 0.1, 1.0)  # Limit max y to 1.0\n",
    "    y_min = -y_max  # Keep symmetric scaling\n",
    "\n",
    "    # Ensure y-axis tick marks are at intervals of 0.05\n",
    "    y_ticks = np.arange(y_min, y_max + 0.05, 0.05)  # Generate ticks\n",
    "\n",
    "    # Create bar plot for relative profit deviation comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bar_width = 0.35\n",
    "    indices = np.arange(len(train_ratios))\n",
    "\n",
    "    # Plot bars for train and test ratios, with train on the left for each pair\n",
    "    train_bars = plt.bar(\n",
    "        indices - bar_width / 2,\n",
    "        train_ratios.values(),\n",
    "        bar_width,\n",
    "        label=\"Train Data\",\n",
    "        color=\"salmon\",\n",
    "    )\n",
    "    test_bars = plt.bar(\n",
    "        indices + bar_width / 2,\n",
    "        test_ratios.values(),\n",
    "        bar_width,\n",
    "        label=\"Test Data\",\n",
    "        color=\"skyblue\",\n",
    "    )\n",
    "\n",
    "    # Add baseline line\n",
    "    plt.axhline(y=0, color=\"gray\", linestyle=\"--\", label=\"Baseline (No Opt)\")\n",
    "\n",
    "    # Add labels for each bar\n",
    "    for bar in train_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval,\n",
    "            f\"{yval:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "    for bar in test_bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval,\n",
    "            f\"{yval:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel(\"Strategies\")\n",
    "    plt.ylabel(\"Deviation from Baseline\")\n",
    "    plt.title(\"Relative Profit Deviation Comparison between Train and Test Data\")\n",
    "    plt.xticks(indices, train_ratios.keys())\n",
    "\n",
    "    # Set fixed y-axis range and ticks\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.yticks(y_ticks)  # Apply fixed 0.05 intervals\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    name = \"plot_relative_profit_comparison\"\n",
    "\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    save_path = f\"plots/{name}_{save_type}.png\"\n",
    "\n",
    "    plt.savefig(save_path, format=\"png\", bbox_inches=\"tight\")\n",
    "    print(f\"Plot saved as {save_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Q0_Q1_distribution(save_type, stimulation_results_dfs):\n",
    "\n",
    "    for idx, df in enumerate(stimulation_results_dfs, start=1):\n",
    "        if df is None or len(df) == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"Q0\"] = pd.to_numeric(df[\"Q0\"], errors=\"coerce\")\n",
    "        df[\"Q1\"] = pd.to_numeric(df[\"Q1\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"Q0\", \"Q1\"], inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df[\"Q0\"], bins=20, alpha=0.6, label=\"Q0\", edgecolor=\"black\")\n",
    "        plt.hist(df[\"Q1\"], bins=20, alpha=0.6, label=\"Q1\", edgecolor=\"black\")\n",
    "        plt.title(f\"Histogram of Q0 and Q1 for stimulation_results_df_{idx}\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        name = \"plot_Q0_Q1_distribution\"\n",
    "\n",
    "        os.makedirs(\"plots\", exist_ok=True)\n",
    "        save_path = (\n",
    "            f\"plots/{name}_{save_type}_{data_size}_S{idx}_{CURRENT_TIMESTAMP}.png\"\n",
    "        )\n",
    "\n",
    "        plt.savefig(save_path, format=\"png\", bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved as {save_path}\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_profits_deviation_box_plot(\n",
    "    save_type, stimulation_results_dfs, baseline_avg_profits\n",
    "):\n",
    "\n",
    "    for idx, df in enumerate(stimulation_results_dfs, start=1):\n",
    "        if df is not None and \"profits\" in df.columns:\n",
    "            df[\"profits\"] = pd.to_numeric(df[\"profits\"], errors=\"coerce\")\n",
    "            df.dropna(subset=[\"profits\"], inplace=True)\n",
    "\n",
    "            # Calculate deviation\n",
    "            df[\"Deviation\"] = df[\"profits\"] - baseline_avg_profits\n",
    "\n",
    "            # Plot deviation as a boxplot\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.boxplot(y=df[\"Deviation\"])\n",
    "            plt.axhline(0, color=\"red\", linestyle=\"--\", label=\"Baseline\")\n",
    "            plt.title(\n",
    "                f\"Boxplot of Deviation of Profits from Baseline for stimulation_results_df_{idx}\"\n",
    "            )\n",
    "            plt.ylabel(\"Deviation\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, axis=\"y\")\n",
    "\n",
    "            name = \"plot_profits_deviation_box_plot\"\n",
    "\n",
    "            os.makedirs(\"plots\", exist_ok=True)\n",
    "            save_path = (\n",
    "                f\"plots/{name}_{save_type}_{data_size}_S{idx}_{CURRENT_TIMESTAMP}.png\"\n",
    "            )\n",
    "\n",
    "            plt.savefig(save_path, format=\"png\", bbox_inches=\"tight\")\n",
    "            print(f\"Plot saved as {save_path}\")\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Skipping stimulation_results_df_{idx}: Missing 'profits' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "id": "96OwNkWqLEx1"
   },
   "outputs": [],
   "source": [
    "# Function to replace negative values with 0\n",
    "def replace_negative_with_zero(df):\n",
    "    return df.applymap(lambda x: max(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "id": "ruyH6M8Jc5yW"
   },
   "outputs": [],
   "source": [
    "def check_values(\n",
    "    Q1_vars,\n",
    "    Q_hat_adjusteds,\n",
    "    Q0_vars,\n",
    "    Sold_0s,\n",
    "    total_demand_up_to_k_minus_1_vars,\n",
    "    Sold_1s,\n",
    "    total_demand_from_k_to_T_vars,\n",
    "    Q1_plus_lefts,\n",
    "    Left_0s,\n",
    "    Lost_0s,\n",
    "    Left_1s,\n",
    "    Lost_1s,\n",
    "):\n",
    "\n",
    "    # ç”¨æ–¼å­˜å„²æ¯å€‹æ¢ä»¶çš„çµ±è¨ˆçµæœ\n",
    "    results = {\n",
    "        \"Condition\": [],\n",
    "        \"Average_Error_Percentage\": [],\n",
    "        \"Max_Error_Percentage\": [],\n",
    "        \"Min_Error_Percentage\": [],\n",
    "        \"Max_Error\": [],\n",
    "        \"Min_Error\": [],\n",
    "    }\n",
    "\n",
    "    # å®šç¾©å­˜å„²æ¯å€‹æ¢ä»¶ä¸‹çš„èª¤å·®å’Œèª¤å·®ç™¾åˆ†æ¯”\n",
    "    conditions_errors = {\n",
    "        \"Q1_vars\": [],\n",
    "        \"Sold_0s\": [],\n",
    "        \"Sold_1s\": [],\n",
    "        \"Left_0s\": [],\n",
    "        \"Left_1s\": [],\n",
    "        \"Lost_0s\": [],\n",
    "        \"Lost_1s\": [],\n",
    "    }\n",
    "\n",
    "    # å­˜å„²æ¯å€‹æ¢ä»¶ä¸‹çš„èª¤å·®ç™¾åˆ†æ¯”\n",
    "    conditions_error_percentage = {\n",
    "        \"Q1_vars\": [],\n",
    "        \"Sold_0s\": [],\n",
    "        \"Sold_1s\": [],\n",
    "        \"Left_0s\": [],\n",
    "        \"Left_1s\": [],\n",
    "        \"Lost_0s\": [],\n",
    "        \"Lost_1s\": [],\n",
    "    }\n",
    "\n",
    "    # éæ­·æ¯ä¸€å€‹è®Šé‡é›†åˆ\n",
    "    for i in range(len(Q1_vars)):\n",
    "        # æå–è®Šé‡çš„å€¼\n",
    "        Q1 = Q1_vars[i].X\n",
    "        Q_hat_adjusted = Q_hat_adjusteds[i].X\n",
    "        Q0 = Q0_vars[i].X\n",
    "        Sold_0 = Sold_0s[i].X\n",
    "        total_demand_up_to_k_minus_1 = total_demand_up_to_k_minus_1_vars[i].X\n",
    "        Sold_1 = Sold_1s[i].X\n",
    "        total_demand_from_k_to_T = total_demand_from_k_to_T_vars[i].X\n",
    "        Q1_plus_left = Q1_plus_lefts[i].X\n",
    "        Left_0 = Left_0s[i].X\n",
    "        Lost_0 = Lost_0s[i].X\n",
    "        Left_1 = Left_1s[i].X\n",
    "        Lost_1 = Lost_1s[i].X\n",
    "\n",
    "        # è¨ˆç®—ç†è«–å€¼\n",
    "        theoretical_sold_0 = min(total_demand_up_to_k_minus_1, Q0)\n",
    "        theoretical_left_0 = max(Q0 - theoretical_sold_0, 0)\n",
    "        theoretical_Q1_plus_left = Q1 + theoretical_left_0  # Q1_plus_left çš„ç†è«–å€¼\n",
    "        theoretical_sold_1 = min(total_demand_from_k_to_T, theoretical_Q1_plus_left)\n",
    "        theoretical_left_1 = max(theoretical_Q1_plus_left - theoretical_sold_1, 0)\n",
    "        theoretical_lost_0 = max(total_demand_up_to_k_minus_1 - Q0, 0)\n",
    "        theoretical_lost_1 = max(total_demand_from_k_to_T - theoretical_Q1_plus_left, 0)\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 2ï¼šSold_0 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Sold_0 == theoretical_sold_0):\n",
    "            error = abs(Sold_0 - theoretical_sold_0)\n",
    "            conditions_errors[\"Sold_0s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Sold_0s\"].append(\n",
    "                (error / theoretical_sold_0) * 100 if theoretical_sold_0 != 0 else 0\n",
    "            )\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 3ï¼šSold_1 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Sold_1 == theoretical_sold_1):\n",
    "            error = abs(Sold_1 - theoretical_sold_1)\n",
    "            conditions_errors[\"Sold_1s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Sold_1s\"].append(\n",
    "                (error / theoretical_sold_1) * 100 if theoretical_sold_1 != 0 else 0\n",
    "            )\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 4ï¼šLeft_0 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Left_0 == theoretical_left_0):\n",
    "            error = abs(Left_0 - theoretical_left_0)\n",
    "            conditions_errors[\"Left_0s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Left_0s\"].append(\n",
    "                (error / theoretical_left_0) * 100 if theoretical_left_0 != 0 else 0\n",
    "            )\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 5ï¼šLeft_1 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Left_1 == theoretical_left_1):\n",
    "            error = abs(Left_1 - theoretical_left_1)\n",
    "            conditions_errors[\"Left_1s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Left_1s\"].append(\n",
    "                (error / theoretical_left_1) * 100 if theoretical_left_1 != 0 else 0\n",
    "            )\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 6ï¼šLost_0 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Lost_0 == theoretical_lost_0):\n",
    "            error = abs(Lost_0 - theoretical_lost_0)\n",
    "            conditions_errors[\"Lost_0s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Lost_0s\"].append(\n",
    "                (error / theoretical_lost_0) * 100 if theoretical_lost_0 != 0 else 0\n",
    "            )\n",
    "\n",
    "        # æª¢æŸ¥æ¢ä»¶ 7ï¼šLost_1 ä¸€å®šç­‰æ–¼ç†è«–å€¼\n",
    "        if not (Lost_1 == theoretical_lost_1):\n",
    "            error = abs(Lost_1 - theoretical_lost_1)\n",
    "            conditions_errors[\"Lost_1s\"].append(error)\n",
    "            # è¨ˆç®—èª¤å·®ç™¾åˆ†æ¯”\n",
    "            conditions_error_percentage[\"Lost_1s\"].append(\n",
    "                (error / theoretical_lost_1) * 100 if theoretical_lost_1 != 0 else 0\n",
    "            )\n",
    "\n",
    "    # è¨ˆç®—æ¯å€‹æ¢ä»¶çš„çµ±è¨ˆçµæœ\n",
    "    for condition, errors in conditions_errors.items():\n",
    "        error_percentages = conditions_error_percentage[condition]\n",
    "        if errors:\n",
    "            # çµ±è¨ˆæ•¸æ“šï¼Œä¸¦å°‡æ‰€æœ‰æ•¸å€¼å››æ¨äº”å…¥è‡³å°æ•¸é»åä¸‰ä½\n",
    "            avg_error_percentage = (\n",
    "                round(sum(error_percentages) / len(error_percentages), 3)\n",
    "                if error_percentages\n",
    "                else 0.0\n",
    "            )\n",
    "            max_error_percentage = (\n",
    "                round(max(error_percentages), 3) if error_percentages else 0.0\n",
    "            )\n",
    "            min_error_percentage = (\n",
    "                round(min(error_percentages), 3) if error_percentages else 0.0\n",
    "            )\n",
    "            max_error = round(max(errors), 3) if errors else 0.0\n",
    "            min_error = round(min(errors), 3) if errors else 0.0\n",
    "\n",
    "            # å­˜å„²çµæœ\n",
    "            results[\"Condition\"].append(condition)\n",
    "            results[\"Average_Error_Percentage\"].append(avg_error_percentage)\n",
    "            results[\"Max_Error_Percentage\"].append(max_error_percentage)\n",
    "            results[\"Min_Error_Percentage\"].append(min_error_percentage)\n",
    "            results[\"Max_Error\"].append(max_error)\n",
    "            results[\"Min_Error\"].append(min_error)\n",
    "\n",
    "    # è½‰æ›ç‚º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "id": "eNMFp-eLU956"
   },
   "outputs": [],
   "source": [
    "# Calculate service level\n",
    "def calculate_service_level(*, salvage_value, cost, price):\n",
    "\n",
    "    cu = price - cost\n",
    "    co = cost - salvage_value\n",
    "    service_lv = cu / (co + cu)\n",
    "\n",
    "    return service_lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_s3_related_strtegies_result(\n",
    "    *,\n",
    "    all_Rs,\n",
    "    losses,\n",
    "    lefts,\n",
    "    profits,\n",
    "    operation_profits,\n",
    "    alpha_values,\n",
    "    beta_values,\n",
    "    F_vars,\n",
    "    Q0_vars,\n",
    "    Q1_vars,\n",
    "    f_values,\n",
    "    tau_values,\n",
    "    holding_costs_0s,\n",
    "    holding_costs_1s,\n",
    "    all_left0s,\n",
    "    all_left1s,\n",
    "    all_lost0s,\n",
    "    all_lost1s,\n",
    "    gamma_values=None\n",
    "):\n",
    "\n",
    "    results_dict = {\n",
    "        \"average_profits\": [sum(profits) / len(profits) if profits else 0],\n",
    "        \"average_losses\": [sum(losses) / len(losses) if losses else 0],\n",
    "        \"average_lefts\": [sum(lefts) / len(lefts) if lefts else 0],\n",
    "        \"average_operation_profits\": [\n",
    "            sum(operation_profits) / len(operation_profits) if operation_profits else 0\n",
    "        ],\n",
    "        \"alpha_values\": [alpha_values],\n",
    "        \"beta_values\": [beta_values],\n",
    "        \"tau_values\": [tau_values],\n",
    "        \"gamma_values\": [gamma_values],\n",
    "    }\n",
    "    stimulations_result = {\n",
    "        \"R(T)\": all_Rs,\n",
    "        \"R\": [x - 2 for x in all_Rs],\n",
    "        \"F\": F_vars,\n",
    "        \"f_values\": f_values,\n",
    "        \"profits\": profits,\n",
    "        \"losses\": losses,\n",
    "        \"lefts\": lefts,\n",
    "        \"operation_profits\": operation_profits,\n",
    "        \"Q0\": Q0_vars,\n",
    "        \"Q1\": Q1_vars,\n",
    "        \"hc0\": holding_costs_0s,\n",
    "        \"hc1\": holding_costs_1s,\n",
    "        \"Left0s\": all_left0s,\n",
    "        \"Left1s\": all_left1s,\n",
    "        \"lost0s\": all_lost0s,\n",
    "        \"lost1s\": all_lost1s,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(results_dict).sort_values(\n",
    "        by=\"average_profits\", ascending=False\n",
    "    ), pd.DataFrame(stimulations_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_chunk100_lasso10_fold10.csv\n",
      "test_chunk70_lasso10_fold10.csv\n",
      "train_chunk50_lasso100_fold10.csv\n",
      "test_chunk50_lasso10_fold10.csv\n",
      "test_chunk70_lasso100_fold10.csv\n",
      "test_chunk100_lasso100_fold3.csv\n",
      "test_chunk70_lasso10_fold3.csv\n",
      "test_chunk70_lasso1000_fold5.csv\n",
      "test_chunk50_lasso100_fold5.csv\n",
      "test_chunk100_lasso10_fold3.csv\n",
      "train_chunk50_lasso10_fold5.csv\n",
      "test_chunk70_lasso100_fold5.csv\n",
      "test_chunk70_lasso1000_fold10.csv\n",
      "train_chunk50_lasso1000_fold5.csv\n",
      "train_chunk70_lasso1000_fold10.csv\n",
      "test_chunk70_lasso10_fold5.csv\n",
      "test_chunk100_lasso100_fold5.csv\n",
      "test_chunk70_lasso1000_fold3.csv\n",
      "test_chunk100_lasso100_fold10.csv\n",
      "test_chunk50_lasso100_fold3.csv\n",
      "test_chunk100_lasso10_fold5.csv\n",
      "train_chunk100_lasso100_fold10.csv\n",
      "train_chunk50_lasso10_fold3.csv\n",
      "test_chunk50_lasso1000_fold10.csv\n",
      "test_chunk70_lasso100_fold3.csv\n",
      "train_chunk50_lasso1000_fold10.csv\n",
      "train_chunk50_lasso1000_fold3.csv\n",
      "train_chunk70_lasso100_fold10.csv\n",
      "train_chunk50_lasso100_fold3.csv\n",
      "train_chunk100_lasso10_fold5.csv\n",
      "test_chunk50_lasso10_fold3.csv\n",
      "test_chunk50_lasso100_fold10.csv\n",
      "train_chunk70_lasso100_fold3.csv\n",
      "train_chunk70_lasso10_fold5.csv\n",
      "train_chunk100_lasso10_fold10.csv\n",
      "train_chunk50_lasso100_fold5.csv\n",
      "train_chunk100_lasso10_fold3.csv\n",
      "test_chunk50_lasso10_fold5.csv\n",
      "train_chunk70_lasso100_fold5.csv\n",
      "train_chunk70_lasso10_fold3.csv\n",
      "train_chunk70_lasso10_fold10.csv\n",
      "test_chunk50_lasso1000_fold3.csv\n",
      "train_chunk70_lasso1000_fold3.csv\n",
      "train_chunk100_lasso100_fold5.csv\n",
      "train_chunk50_lasso10_fold10.csv\n",
      "test_chunk50_lasso1000_fold5.csv\n",
      "train_chunk70_lasso1000_fold5.csv\n",
      "train_chunk100_lasso100_fold3.csv\n"
     ]
    }
   ],
   "source": [
    "# åˆ—å‡ºæ‰€æœ‰ csv\n",
    "import os\n",
    "\n",
    "# ç›®éŒ„è·¯å¾‘\n",
    "directory = \"/Users/hanyuan/Github/Two-Phase-Newsvendor/results\"\n",
    "\n",
    "# åˆ—å‡ºæ‰€æœ‰ .csv æª”æ¡ˆ\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "for file in csv_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Matched train-test pairs:\n",
      " - chunk100_lasso1_fold10.csv\n",
      " - chunk50_lasso1_fold10.csv\n",
      " - chunk70_lasso1_fold10.csv\n",
      "\n",
      "âŒ Only in train:\n",
      "\n",
      "âŒ Only in test:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# è³‡æ–™å¤¾è·¯å¾‘\n",
    "directory = \"/Users/hanyuan/Github/Two-Phase-Newsvendor/results_0327\"\n",
    "\n",
    "# å–å¾—æ‰€æœ‰ .csv æª”æ¡ˆåç¨±\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# ç”¨ä¾†å­˜çµ„åˆéµ\n",
    "train_keys = set()\n",
    "test_keys = set()\n",
    "\n",
    "# åˆ†ææ¯å€‹æª”æ¡ˆ\n",
    "for filename in csv_files:\n",
    "    if filename.startswith(\"train_\"):\n",
    "        key = filename.replace(\"train_\", \"\")\n",
    "        train_keys.add(key)\n",
    "    elif filename.startswith(\"test_\"):\n",
    "        key = filename.replace(\"test_\", \"\")\n",
    "        test_keys.add(key)\n",
    "\n",
    "# æ‰¾å‡ºåªæœ‰åœ¨ train æœ‰çš„çµ„åˆ\n",
    "train_only = train_keys - test_keys\n",
    "\n",
    "# æ‰¾å‡ºåªæœ‰åœ¨ test æœ‰çš„çµ„åˆ\n",
    "test_only = test_keys - train_keys\n",
    "\n",
    "# æ‰¾å‡ºåŒæ™‚æœ‰ train å’Œ test çš„çµ„åˆ\n",
    "matched = train_keys & test_keys\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"âœ… Matched train-test pairs:\")\n",
    "for key in sorted(matched):\n",
    "    print(f\" - {key}\")\n",
    "\n",
    "print(\"\\nâŒ Only in train:\")\n",
    "for key in sorted(train_only):\n",
    "    print(f\" - {key}\")\n",
    "\n",
    "print(\"\\nâŒ Only in test:\")\n",
    "for key in sorted(test_only):\n",
    "    print(f\" - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 matched pairs.\n",
      "-----------------------------------\n",
      "key: chunk100_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk100_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk100_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk100_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk200_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk200_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk200_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk200_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk20_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk20_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk20_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk20_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk300_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk300_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk300_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk300_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk400_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk400_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk400_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk400_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk40_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk40_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk40_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk40_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk50_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk50_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk50_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk50_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk60_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk60_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk60_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk60_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk70_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk70_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk70_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk70_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk80_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk80_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk80_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk80_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk90_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk90_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk90_lasso1_fold10.csv\n",
      "âœ… Saved plot for: chunk90_lasso1_fold10.csv\n",
      "\n",
      "ğŸ‰ All plots saved in: /Users/hanyuan/Github/Two-Phase-Newsvendor/results_0327/plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è¨­å®š seaborn æ¨£å¼\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ğŸ”§ åƒæ•¸è¨­å®š\n",
    "input_dir = \"/Users/hanyuan/Github/Two-Phase-Newsvendor/results_0327\"\n",
    "output_dir = os.path.join(input_dir, \"plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # å¦‚æœ plots è³‡æ–™å¤¾ä¸å­˜åœ¨å°±å»ºç«‹\n",
    "\n",
    "# ğŸ” å–å¾—æ‰€æœ‰ train/test æª”æ¡ˆ\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\")]\n",
    "train_files = {f.replace(\"train_\", \"\"): f for f in files if f.startswith(\"train_\")}\n",
    "test_files = {f.replace(\"test_\", \"\"): f for f in files if f.startswith(\"test_\")}\n",
    "\n",
    "# ğŸ”— æ‰¾å‡ºäº¤é›†çµ„åˆéµ\n",
    "common_keys = sorted(set(train_files.keys()) & set(test_files.keys()))\n",
    "\n",
    "print(f\"Found {len(common_keys)} matched pairs.\")\n",
    "\n",
    "# ğŸ¯ å°æ¯çµ„é…å°åšåˆ†æå’Œç•«åœ–\n",
    "for key in common_keys:\n",
    "    print(f\"-----------------------------------\")\n",
    "    print(f\"key: {key}\")\n",
    "    print(f\"train_files[key]: {train_files[key]}\")\n",
    "    print(f\"test_files[key]: {test_files[key]}\")\n",
    "\n",
    "    train_path = os.path.join(input_dir, train_files[key])\n",
    "    test_path = os.path.join(input_dir, test_files[key])\n",
    "\n",
    "    # è®€å– CSV æª”æ¡ˆ\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_df = train_df.drop(columns=[\"S12\", \"S15\"])\n",
    "\n",
    "    # è¨ˆç®—å¹³å‡\n",
    "    train_means = train_df.mean()\n",
    "    test_means = test_df.mean()\n",
    "\n",
    "    # baseline å’Œ theory æœ€ä½³\n",
    "    baseline_train = train_means[\"baseline\"]\n",
    "    baseline_test = test_means[\"baseline\"]\n",
    "    theory_best_train = train_means[\"S14\"]\n",
    "    theory_best_test = test_means[\"S14\"]\n",
    "\n",
    "    # ç™¾åˆ†æ¯”è®ŠåŒ–\n",
    "    train_pct_base = (train_means - baseline_train) / baseline_train * 100\n",
    "    test_pct_base = (test_means - baseline_test) / baseline_test * 100\n",
    "    train_pct_theory = (train_means - theory_best_train) / theory_best_train * 100\n",
    "    test_pct_theory = (test_means - theory_best_test) / theory_best_test * 100\n",
    "\n",
    "    # æ•´ç†æˆ DataFrame\n",
    "    avg_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Method\": train_means.index,\n",
    "            \"Train\": train_means.values,\n",
    "            \"Test\": test_means.values,\n",
    "            \"Train_%_Base\": train_pct_base.values,\n",
    "            \"Test_%_Base\": test_pct_base.values,\n",
    "            \"Train_%_Theory\": train_pct_theory.values,\n",
    "            \"Test_%_Theory\": test_pct_theory.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # é•·æ ¼å¼è½‰æ›\n",
    "    avg_df_melted = avg_df.melt(\n",
    "        id_vars=[\n",
    "            \"Method\",\n",
    "            \"Train_%_Base\",\n",
    "            \"Test_%_Base\",\n",
    "            \"Train_%_Theory\",\n",
    "            \"Test_%_Theory\",\n",
    "        ],\n",
    "        value_vars=[\"Train\", \"Test\"],\n",
    "        var_name=\"Dataset\",\n",
    "        value_name=\"Average Profit\",\n",
    "    )\n",
    "\n",
    "    # ç•«åœ–\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    ax = sns.barplot(x=\"Method\", y=\"Average Profit\", hue=\"Dataset\", data=avg_df_melted)\n",
    "\n",
    "    # åŠ ä¸Šè¨»è§£\n",
    "    for patch, (method, ds) in zip(\n",
    "        ax.patches, zip(avg_df_melted[\"Method\"], avg_df_melted[\"Dataset\"])\n",
    "    ):\n",
    "        if ds == \"Train\":\n",
    "            pct_base = avg_df.loc[avg_df.Method == method, \"Train_%_Base\"].values[0]\n",
    "            pct_theory = avg_df.loc[avg_df.Method == method, \"Train_%_Theory\"].values[0]\n",
    "        else:\n",
    "            pct_base = avg_df.loc[avg_df.Method == method, \"Test_%_Base\"].values[0]\n",
    "            pct_theory = avg_df.loc[avg_df.Method == method, \"Test_%_Theory\"].values[0]\n",
    "\n",
    "        ax.annotate(\n",
    "            f\"{pct_base:.1f}%\\n({pct_theory:.1f}%)\",\n",
    "            (patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Average Profit: {key}\\n% Change vs Baseline / Theory Best\")\n",
    "    plt.ylabel(\"Average Profit\")\n",
    "    plt.xlabel(\"Method\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # å„²å­˜åœ–ç‰‡\n",
    "    plot_filename = os.path.join(output_dir, f\"avg_profit_{key.replace('.csv','')}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ… Saved plot for: {key}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ All plots saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 matched pairs.\n",
      "-----------------------------------\n",
      "key: chunk100_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk100_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk100_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk100_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk200_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk200_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk200_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk200_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk20_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk20_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk20_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk20_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk300_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk300_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk300_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk300_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk400_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk400_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk400_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk400_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk40_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk40_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk40_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk40_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk50_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk50_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk50_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk50_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk60_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk60_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk60_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk60_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk70_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk70_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk70_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk70_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk80_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk80_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk80_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk80_lasso1_fold10.csv\n",
      "-----------------------------------\n",
      "key: chunk90_lasso1_fold10.csv\n",
      "train_files[key]: train_chunk90_lasso1_fold10.csv\n",
      "test_files[key]: test_chunk90_lasso1_fold10.csv\n",
      "âœ… Plots saved for: chunk90_lasso1_fold10.csv\n",
      "\n",
      "ğŸ‰ All plots saved to: /Users/hanyuan/Github/Two-Phase-Newsvendor/results_0327/plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è¨­å®š seaborn æ¨£å¼\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ğŸ”§ åƒæ•¸è¨­å®š\n",
    "input_dir = \"/Users/hanyuan/Github/Two-Phase-Newsvendor/results_0327\"\n",
    "output_dir = os.path.join(input_dir, \"plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # å¦‚æœ plots è³‡æ–™å¤¾ä¸å­˜åœ¨å°±å»ºç«‹\n",
    "\n",
    "# ğŸ” å–å¾—æ‰€æœ‰ train/test æª”æ¡ˆ\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\")]\n",
    "train_files = {f.replace(\"train_\", \"\"): f for f in files if f.startswith(\"train_\")}\n",
    "test_files = {f.replace(\"test_\", \"\"): f for f in files if f.startswith(\"test_\")}\n",
    "\n",
    "# ğŸ”— æ‰¾å‡ºäº¤é›†çµ„åˆéµ\n",
    "common_keys = sorted(set(train_files.keys()) & set(test_files.keys()))\n",
    "\n",
    "print(f\"Found {len(common_keys)} matched pairs.\")\n",
    "\n",
    "# ğŸ¯ å°æ¯çµ„é…å°åšåˆ†æå’Œç•«åœ–\n",
    "for key in common_keys:\n",
    "    print(f\"-----------------------------------\")\n",
    "    print(f\"key: {key}\")\n",
    "    print(f\"train_files[key]: {train_files[key]}\")\n",
    "    print(f\"test_files[key]: {test_files[key]}\")\n",
    "\n",
    "    train_path = os.path.join(input_dir, train_files[key])\n",
    "    test_path = os.path.join(input_dir, test_files[key])\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    train_df = train_df.drop(columns=[\"S12\", \"S15\"])\n",
    "\n",
    "    # ========== 1ï¸âƒ£ è¨ˆç®— Relative (%) vs Baseline ==========\n",
    "    baseline_train = train_df[\"baseline\"].to_numpy().reshape(-1, 1)\n",
    "    baseline_test = test_df[\"baseline\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    train_relative = (train_df.to_numpy() - baseline_train) / baseline_train * 100\n",
    "    test_relative = (test_df.to_numpy() - baseline_test) / baseline_test * 100\n",
    "\n",
    "    train_relative = pd.DataFrame(\n",
    "        train_relative, columns=train_df.columns, index=train_df.index\n",
    "    )\n",
    "    test_relative = pd.DataFrame(\n",
    "        test_relative, columns=test_df.columns, index=test_df.index\n",
    "    )\n",
    "\n",
    "    train_relative[\"Fold\"] = train_relative.index + 1\n",
    "    test_relative[\"Fold\"] = test_relative.index + 1\n",
    "\n",
    "    train_long = train_relative.melt(\n",
    "        id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative Profit (%)\"\n",
    "    )\n",
    "    test_long = test_relative.melt(\n",
    "        id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative Profit (%)\"\n",
    "    )\n",
    "    train_long[\"Dataset\"] = \"Train\"\n",
    "    test_long[\"Dataset\"] = \"Test\"\n",
    "\n",
    "    fold_long = pd.concat([train_long, test_long], axis=0)\n",
    "    fold_long = fold_long.reset_index(drop=True)\n",
    "\n",
    "    # ğŸ¨ 1. Line Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=fold_long,\n",
    "        x=\"Fold\",\n",
    "        y=\"Relative Profit (%)\",\n",
    "        hue=\"Method\",\n",
    "        style=\"Dataset\",\n",
    "        markers=True,\n",
    "        dashes=False,\n",
    "    )\n",
    "    plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    plt.title(f\"[{key}] Strategy Performance Across Folds (Relative to Baseline)\")\n",
    "    plt.legend(title=\"Method & Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"line_{key.replace('.csv','')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ğŸ¨ 2. Box Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=fold_long, x=\"Method\", y=\"Relative Profit (%)\", hue=\"Dataset\")\n",
    "    plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    plt.title(f\"[{key}] Strategy Performance Distribution Across Folds\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"box_{key.replace('.csv','')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ========== 2ï¸âƒ£ Heatmap: Relative vs Theory ==========\n",
    "    theory_best_train = train_df[\"S14\"].to_numpy().reshape(-1, 1)\n",
    "    theory_best_test = test_df[\"S14\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    train_theory_rel = (\n",
    "        (train_df.to_numpy() - theory_best_train) / theory_best_train * 100\n",
    "    )\n",
    "    test_theory_rel = (test_df.to_numpy() - theory_best_test) / theory_best_test * 100\n",
    "\n",
    "    train_theory_df = pd.DataFrame(\n",
    "        train_theory_rel, columns=train_df.columns, index=train_df.index\n",
    "    )\n",
    "    test_theory_df = pd.DataFrame(\n",
    "        test_theory_rel, columns=test_df.columns, index=test_df.index\n",
    "    )\n",
    "\n",
    "    train_theory_df[\"Fold\"] = train_theory_df.index + 1\n",
    "    test_theory_df[\"Fold\"] = test_theory_df.index + 1\n",
    "\n",
    "    train_theory_long = train_theory_df.melt(\n",
    "        id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative vs Theory (%)\"\n",
    "    )\n",
    "    train_theory_long[\"Dataset\"] = \"Train\"\n",
    "    test_theory_long = test_theory_df.melt(\n",
    "        id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative vs Theory (%)\"\n",
    "    )\n",
    "    test_theory_long[\"Dataset\"] = \"Test\"\n",
    "\n",
    "    merged = fold_long.merge(\n",
    "        pd.concat([train_theory_long, test_theory_long], axis=0),\n",
    "        on=[\"Fold\", \"Method\", \"Dataset\"],\n",
    "    )\n",
    "\n",
    "    heatmap_data = merged.pivot(\n",
    "        index=\"Fold\", columns=[\"Method\", \"Dataset\"], values=\"Relative Profit (%)\"\n",
    "    )\n",
    "    annot = merged.assign(\n",
    "        annot=merged[\"Relative Profit (%)\"].round(1).astype(str)\n",
    "        + \"\\n(\"\n",
    "        + merged[\"Relative vs Theory (%)\"].round(1).astype(str)\n",
    "        + \"%)\"\n",
    "    ).pivot(index=\"Fold\", columns=[\"Method\", \"Dataset\"], values=\"annot\")\n",
    "\n",
    "    # ğŸ¨ 3. Heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(heatmap_data, annot=annot, fmt=\"\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "    plt.title(\n",
    "        f\"[{key}] Relative Profit (%) Across Folds\\n(vs Baseline / (vs Theory Best))\"\n",
    "    )\n",
    "    plt.ylabel(\"Fold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"heatmap_{key.replace('.csv','')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ… Plots saved for: {key}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ All plots saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1ï¸âƒ£ è¨ˆç®—å¹³å‡ profit\n",
    "# train_means = train_all_fold_profit_df.mean()\n",
    "# test_means = test_all_fold_profit_df.mean()\n",
    "\n",
    "# # 2ï¸âƒ£ å®šç¾© baseline & theory best\n",
    "# baseline_train = train_means[\"baseline\"]\n",
    "# baseline_test = test_means[\"baseline\"]\n",
    "# theory_best_train = train_means[\"S14\"]\n",
    "# theory_best_test = test_means[\"S14\"]\n",
    "\n",
    "# # 3ï¸âƒ£ è¨ˆç®—ç™¾åˆ†æ¯”è®ŠåŒ–ï¼šbaseline & theory\n",
    "# train_pct_base = (train_means - baseline_train) / baseline_train * 100\n",
    "# test_pct_base = (test_means - baseline_test) / baseline_test * 100\n",
    "# train_pct_theory = (train_means - theory_best_train) / theory_best_train * 100\n",
    "# test_pct_theory = (test_means - theory_best_test) / theory_best_test * 100\n",
    "\n",
    "# # 4ï¸âƒ£ å»º DataFrame\n",
    "# avg_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Method\": train_means.index,\n",
    "#         \"Train\": train_means.values,\n",
    "#         \"Test\": test_means.values,\n",
    "#         \"Train_%_Base\": train_pct_base.values,\n",
    "#         \"Test_%_Base\": test_pct_base.values,\n",
    "#         \"Train_%_Theory\": train_pct_theory.values,\n",
    "#         \"Test_%_Theory\": test_pct_theory.values,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# avg_df_melted = avg_df.melt(\n",
    "#     id_vars=[\n",
    "#         \"Method\",\n",
    "#         \"Train_%_Base\",\n",
    "#         \"Test_%_Base\",\n",
    "#         \"Train_%_Theory\",\n",
    "#         \"Test_%_Theory\",\n",
    "#     ],\n",
    "#     value_vars=[\"Train\", \"Test\"],\n",
    "#     var_name=\"Dataset\",\n",
    "#     value_name=\"Average Profit\",\n",
    "# )\n",
    "\n",
    "# # 5ï¸âƒ£ ç•«åœ–\n",
    "# plt.figure(figsize=(15, 9))\n",
    "# ax = sns.barplot(x=\"Method\", y=\"Average Profit\", hue=\"Dataset\", data=avg_df_melted)\n",
    "\n",
    "# # 6ï¸âƒ£ æ¨™è¨»ï¼šbaseline (%) åœ¨ç¬¬ä¸€è¡Œã€theory (%) æ‹¬è™Ÿå…§ç¬¬äºŒè¡Œ\n",
    "# for patch, (method, ds) in zip(\n",
    "#     ax.patches, zip(avg_df_melted[\"Method\"], avg_df_melted[\"Dataset\"])\n",
    "# ):\n",
    "#     if ds == \"Train\":\n",
    "#         pct_base = avg_df.loc[avg_df.Method == method, \"Train_%_Base\"].values[0]\n",
    "#         pct_theory = avg_df.loc[avg_df.Method == method, \"Train_%_Theory\"].values[0]\n",
    "#     else:\n",
    "#         pct_base = avg_df.loc[avg_df.Method == method, \"Test_%_Base\"].values[0]\n",
    "#         pct_theory = avg_df.loc[avg_df.Method == method, \"Test_%_Theory\"].values[0]\n",
    "\n",
    "#     ax.annotate(\n",
    "#         f\"{pct_base:.1f}%\\n({pct_theory:.1f}%)\",\n",
    "#         (patch.get_x() + patch.get_width() / 2, patch.get_height()),\n",
    "#         ha=\"center\",\n",
    "#         va=\"bottom\",\n",
    "#         fontsize=9,\n",
    "#         xytext=(0, 5),\n",
    "#         textcoords=\"offset points\",\n",
    "#     )\n",
    "\n",
    "# plt.title(\"Average Profit (Train vs Test) â€” % Change vs Baseline / Theory Best\")\n",
    "# plt.ylabel(\"Average Profit\")\n",
    "# plt.xlabel(\"Method\")\n",
    "# plt.xticks(rotation=30)\n",
    "# plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # è¨ˆç®— baselineï¼ˆè¨“ç·´èˆ‡æ¸¬è©¦ï¼‰\n",
    "# baseline_train = train_all_fold_profit_df[\"baseline\"].to_numpy().reshape(-1, 1)\n",
    "# baseline_test = test_all_fold_profit_df[\"baseline\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# # è¨ˆç®—ç™¾åˆ†æ¯”è®ŠåŒ–\n",
    "# train_relative = (\n",
    "#     (train_all_fold_profit_df.to_numpy() - baseline_train) / baseline_train * 100\n",
    "# )\n",
    "# test_relative = (\n",
    "#     (test_all_fold_profit_df.to_numpy() - baseline_test) / baseline_test * 100\n",
    "# )\n",
    "\n",
    "# # è½‰å› DataFrameï¼Œä¸¦ä¿ç•™ column names\n",
    "# train_relative = pd.DataFrame(\n",
    "#     train_relative,\n",
    "#     columns=train_all_fold_profit_df.columns,\n",
    "#     index=train_all_fold_profit_df.index,\n",
    "# )\n",
    "# test_relative = pd.DataFrame(\n",
    "#     test_relative,\n",
    "#     columns=test_all_fold_profit_df.columns,\n",
    "#     index=test_all_fold_profit_df.index,\n",
    "# )\n",
    "\n",
    "# # åŠ å…¥ fold ç·¨è™Ÿ\n",
    "# train_relative[\"Fold\"] = train_relative.index + 1\n",
    "# test_relative[\"Fold\"] = test_relative.index + 1\n",
    "\n",
    "# # è½‰æ›æˆé•·æ ¼å¼\n",
    "# train_long = train_relative.melt(\n",
    "#     id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative Profit (%)\"\n",
    "# )\n",
    "# train_long[\"Dataset\"] = \"Train\"\n",
    "\n",
    "# test_long = test_relative.melt(\n",
    "#     id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative Profit (%)\"\n",
    "# )\n",
    "# test_long[\"Dataset\"] = \"Test\"\n",
    "\n",
    "# # åˆä½µæ•¸æ“š\n",
    "# fold_long = pd.concat([train_long, test_long], axis=0)\n",
    "\n",
    "# # === 1. ä½¿ç”¨ç·šåœ– (Line Plot) è§€å¯Ÿä¸åŒ Fold ä¸Šçš„è®ŠåŒ–è¶¨å‹¢ ===\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.lineplot(\n",
    "#     data=fold_long,\n",
    "#     x=\"Fold\",\n",
    "#     y=\"Relative Profit (%)\",\n",
    "#     hue=\"Method\",\n",
    "#     style=\"Dataset\",\n",
    "#     markers=True,\n",
    "#     dashes=False,\n",
    "# )\n",
    "# plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)  # åŸºæº–ç·š\n",
    "# plt.title(\"Strategy Performance Across Folds (Relative to Baseline)\")\n",
    "# plt.legend(title=\"Method & Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "# plt.show()\n",
    "\n",
    "# # === 2. ä½¿ç”¨ç®±å‹åœ– (Box Plot) æŸ¥çœ‹ç­–ç•¥ç©©å®šæ€§ ===\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(data=fold_long, x=\"Method\", y=\"Relative Profit (%)\", hue=\"Dataset\")\n",
    "# plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "# plt.title(\"Strategy Performance Distribution Across Folds\")\n",
    "# plt.xticks(rotation=30)\n",
    "# plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "# plt.show()\n",
    "\n",
    "# # 3ï¸âƒ£ Heatmapï¼šåŒæ™‚é¡¯ç¤º vs Baseline & vs Theory Bestï¼ˆæ¯ Fold çš„ S14ï¼‰\n",
    "# theory_best_train = train_all_fold_profit_df[\"S14\"].to_numpy().reshape(-1, 1)\n",
    "# theory_best_test = test_all_fold_profit_df[\"S14\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# # vs Theory (%) è¨ˆç®—\n",
    "# train_theory_rel = (\n",
    "#     (train_all_fold_profit_df.to_numpy() - theory_best_train) / theory_best_train * 100\n",
    "# )\n",
    "# test_theory_rel = (\n",
    "#     (test_all_fold_profit_df.to_numpy() - theory_best_test) / theory_best_test * 100\n",
    "# )\n",
    "\n",
    "# # å› DataFrame ä¸¦ melt\n",
    "# train_theory_rel = pd.DataFrame(\n",
    "#     train_theory_rel,\n",
    "#     columns=train_all_fold_profit_df.columns,\n",
    "#     index=train_all_fold_profit_df.index,\n",
    "# )\n",
    "# train_theory_rel[\"Fold\"] = train_theory_rel.index + 1\n",
    "# train_theory_long = train_theory_rel.melt(\n",
    "#     id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative vs Theory (%)\"\n",
    "# )\n",
    "# train_theory_long[\"Dataset\"] = \"Train\"\n",
    "\n",
    "# test_theory_rel = pd.DataFrame(\n",
    "#     test_theory_rel,\n",
    "#     columns=test_all_fold_profit_df.columns,\n",
    "#     index=test_all_fold_profit_df.index,\n",
    "# )\n",
    "# test_theory_rel[\"Fold\"] = test_theory_rel.index + 1\n",
    "# test_theory_long = test_theory_rel.melt(\n",
    "#     id_vars=\"Fold\", var_name=\"Method\", value_name=\"Relative vs Theory (%)\"\n",
    "# )\n",
    "# test_theory_long[\"Dataset\"] = \"Test\"\n",
    "\n",
    "# # åˆä½µ baseline (%) èˆ‡ theory (%) è³‡æ–™\n",
    "# merged = fold_long.merge(\n",
    "#     pd.concat([train_theory_long, test_theory_long], axis=0),\n",
    "#     on=[\"Fold\", \"Method\", \"Dataset\"],\n",
    "# )\n",
    "\n",
    "# # Pivot heatmap values + annotations\n",
    "# heatmap_data = merged.pivot(\n",
    "#     index=\"Fold\", columns=[\"Method\", \"Dataset\"], values=\"Relative Profit (%)\"\n",
    "# )\n",
    "# annot = merged.assign(\n",
    "#     annot=merged[\"Relative Profit (%)\"].round(1).astype(str)\n",
    "#     + \"\\n(\"\n",
    "#     + merged[\"Relative vs Theory (%)\"].round(1).astype(str)\n",
    "#     + \"%)\"\n",
    "# ).pivot(index=\"Fold\", columns=[\"Method\", \"Dataset\"], values=\"annot\")\n",
    "\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# sns.heatmap(heatmap_data, annot=annot, fmt=\"\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "# plt.title(\"Relative Profit (%) Across Folds\\n(vs Baseline / (vs Theory Best))\")\n",
    "# plt.ylabel(\"Fold\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # è¨“ç·´éšæ®µåˆ†ä½ˆ\n",
    "\n",
    "# baseline_data = []\n",
    "# S1_data = []\n",
    "# S2_data = []\n",
    "# S12_data = []\n",
    "# S14_data = []\n",
    "# S15_data = []\n",
    "\n",
    "# for result in train_all_fold_stimulation_results:\n",
    "#     baseline_data.append(result[\"baseline\"])\n",
    "#     S1_data.append(result[\"S1\"])\n",
    "#     S2_data.append(result[\"S2\"])\n",
    "#     S12_data.append(result[\"S12\"])\n",
    "#     S14_data.append(result[\"S14\"])\n",
    "#     S15_data.append(result[\"S15\"])\n",
    "\n",
    "# # åˆä½µæ•¸æ“š\n",
    "# baseline_df = pd.concat(baseline_data, ignore_index=True)\n",
    "# S1_df = pd.concat(S1_data, ignore_index=True)\n",
    "# S2_df = pd.concat(S2_data, ignore_index=True)\n",
    "# S12_df = pd.concat(S12_data, ignore_index=True)\n",
    "# S14_df = pd.concat(S14_data, ignore_index=True)\n",
    "# S15_df = pd.concat(S15_data, ignore_index=True)\n",
    "\n",
    "\n",
    "# dfs = {\n",
    "#     \"baseline\": baseline_df,\n",
    "#     \"S1\": S1_df,\n",
    "#     \"S2\": S2_df,\n",
    "#     \"S12\": S12_df,\n",
    "#     \"S15\": S15_df,\n",
    "#     \"S14\": S14_df,\n",
    "# }\n",
    "\n",
    "# # èª¿ç”¨ç¹ªåœ–å‡½æ•¸\n",
    "# plot_strategies_profits_scatter(f\"{status}_{model_prefix}\", dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # æ¸¬è©¦çµæœåˆ†å¸ƒåœ–\n",
    "\n",
    "# baseline_data = []\n",
    "# S1_data = []\n",
    "# S2_data = []\n",
    "# S12_data = []\n",
    "# S14_data = []\n",
    "# S15_data = []\n",
    "\n",
    "# for result in test_all_fold_stimulation_results:\n",
    "#     baseline_data.append(result[\"baseline\"])\n",
    "#     S1_data.append(result[\"S1\"])\n",
    "#     S2_data.append(result[\"S2\"])\n",
    "#     S12_data.append(result[\"S12\"])\n",
    "#     S14_data.append(result[\"S14\"])\n",
    "#     S15_data.append(result[\"S15\"])\n",
    "# # åˆä½µæ•¸æ“š\n",
    "# baseline_df = pd.concat(baseline_data, ignore_index=True)\n",
    "# S1_df = pd.concat(S1_data, ignore_index=True)\n",
    "# S2_df = pd.concat(S2_data, ignore_index=True)\n",
    "# S12_df = pd.concat(S12_data, ignore_index=True)\n",
    "# S14_df = pd.concat(S14_data, ignore_index=True)\n",
    "# S15_df = pd.concat(S15_data, ignore_index=True)\n",
    "\n",
    "# dfs = {\n",
    "#     \"baseline\": baseline_df,\n",
    "#     \"S1\": S1_df,\n",
    "#     \"S2\": S2_df,\n",
    "#     \"S12\": S12_df,\n",
    "#     \"S15\": S15_df,\n",
    "#     \"S14\": S14_df,\n",
    "# }\n",
    "\n",
    "# # èª¿ç”¨ç¹ªåœ–å‡½æ•¸\n",
    "# plot_strategies_profits_scatter(f\"{status}_{model_prefix}\", dfs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BmCtfhVmxlqf",
    "HTASer1nQ6iQ",
    "aRmALsClGzQB",
    "zg9HWiZOypqj",
    "FTJPzWLlAz8L",
    "yXuk_hytiwhv",
    "lQUlr1TGYuqf",
    "uleVduhQ5KpR",
    "igerpH_M5KpT",
    "6EOHpsM05KpT"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
